{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Image captioning with visual attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ομάδα 57:\n",
        "\n",
        "Ανδρέας Χατζησάββας 03118701\n",
        "\n",
        "Θεόδωρος Σωτήρου 03118209\n",
        "\n",
        "Λουκία Παυλανά 03118711"
      ],
      "metadata": {
        "id": "xvWg1-aWQx7m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bwwk4uxRz6A"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc06pTaBbl72",
        "outputId": "23c2c5b8-b7ac-402b-f4a1-bf5fc347a65f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be DOWNGRADED:\n",
            "  libcudnn8\n",
            "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 17 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 1,153 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 7s (57.6 MB/s)\n",
            "(Reading database ... 128208 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.7.0.84-1+cuda11.8) ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.7.0.84-1+cuda11.8 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 128175 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.7.0.84-1+cuda11.8) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R1hQGtZEi8Y",
        "outputId": "6dad76b8-23b7-4005-8b0a-52e3164e8da5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Successfully uninstalled keras-2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xbt8BkPv8Ou",
        "outputId": "6774cee6-d37c-40b4-ff96-4366c2e6926b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.8/dist-packages (4.8.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (5.4.8)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.12.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (5.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (4.64.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (2.25.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.14.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.58.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras, tensorflow, tensorflow_text\n",
            "Successfully installed keras-2.11.0 tensorflow-2.11.0 tensorflow_text-2.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TGZmOuqMia9",
        "outputId": "dd10ca33-b470-482f-c6e5-894b358d587d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "Uuk3uEFbQ1Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "zcBl7LF0Q3Zm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAagBw5p-TM"
      },
      "source": [
        "#### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download image files\n",
        "image_zip = tf.keras.utils.get_file('flickr30k-images-ecemod.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip',\n",
        "                                      extract=True)\n",
        "os.remove(image_zip)"
      ],
      "metadata": {
        "id": "_aDNz2E8PZn8",
        "outputId": "4d995e66-d6a6-407e-cbb1-8dc96c21cd15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip\n",
            "4376381805/4376381805 [==============================] - 426s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download captions file\n",
        "captions_file = tf.keras.utils.get_file('captions_new.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/captions_new.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download train files list\n",
        "train_files_list = tf.keras.utils.get_file('train_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/train_files.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download test files list\n",
        "test_files_list = tf.keras.utils.get_file('test_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/test_files.csv',\n",
        "                                           extract=False)"
      ],
      "metadata": {
        "id": "gQ1oK4dwPa7L",
        "outputId": "8fe08126-3dd8-495c-f287-d9e0553c065e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://spartacus.1337.cx/flickr-mod/captions_new.csv\n",
            "12186945/12186945 [==============================] - 2s 0us/step\n",
            "Downloading data from https://spartacus.1337.cx/flickr-mod/train_files.csv\n",
            "307399/307399 [==============================] - 1s 2us/step\n",
            "Downloading data from https://spartacus.1337.cx/flickr-mod/test_files.csv\n",
            "66196/66196 [==============================] - 0s 4us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path=\".\"\n",
        "IMAGE_DIR=\"image_dir\"\n",
        "path = pathlib.Path(path)\n",
        "   \n",
        "captions = (path/captions_file).read_text().splitlines()\n",
        "captions = (line.split('\\t') for line in captions)\n",
        "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "   \n",
        "cap_dict = collections.defaultdict(list)\n",
        "for fname, cap in captions:\n",
        "  cap_dict[fname].append(cap)\n",
        "   \n",
        "train_files = (path/train_files_list).read_text().splitlines()\n",
        "train_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in train_files]\n",
        "   \n",
        "test_files = (path/test_files_list).read_text().splitlines()\n",
        "test_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in test_files]\n",
        "   \n",
        "train_raw = tf.data.experimental.from_list(train_captions)\n",
        "test_raw = tf.data.experimental.from_list(test_captions)"
      ],
      "metadata": {
        "id": "3C7FyGkzPc2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAQSps5F8RQI",
        "outputId": "6c77af86-55c5-4b7b-81c0-3d3f3c50a129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_raw.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "QD-D7-kjRAY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10000 #5000,6000,7000,8000,9000,10000\n",
        "VOCABULARY_SIZE = 10000\n",
        "\n",
        "#HYPERPARAMETERS\n",
        "MIN_CAPTION_LEN = 4  #3,4,5,6\n",
        "MAX_CAPTION_LEN = 30 #30,35,40,45\n",
        "\n",
        "#EMBEDDINGS\n",
        "#EMBEDDING_DIM = 256 # for False: default 256\n",
        "EMBEDDING_DIM = 300 # for True: 50, 100, 200 or 300"
      ],
      "metadata": {
        "id": "iJJlFNX8RB6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data"
      ],
      "metadata": {
        "id": "SR24WWkJGosx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_to_caption = collections.defaultdict(list)\n",
        "\n",
        "for path, captions in train_captions:\n",
        "    for caption in captions:\n",
        "        image_path_to_caption[path].append(caption)\n",
        "print(image_path_to_caption['image_dir/_3430497.jpg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW-aYJaNG30o",
        "outputId": "c05fe91c-75e8-4e2a-890c-e221e110b25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The skier is wearing a yellow jumpsuit and sliding across a yellow rail .', 'A yellow uniformed skier is performing a trick across a railed object .', 'A skier in electric green on the edge of a ramp made of metal bars .', 'A person on skis on a rail at night .', 'A skier slides along a metal rail .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list with all the paths of the train captions\n",
        "image_paths = [row[0] for row in train_captions]"
      ],
      "metadata": {
        "id": "KXyNrntjHBPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ένα \"καλό\" παράθυρο μήκους για τα captions αποφασίσαμε στο [4,30]."
      ],
      "metadata": {
        "id": "VYYn-ysThAd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_lengths = []\n",
        "train_captions = []\n",
        "\n",
        "for path in image_paths:\n",
        "  caption_list = image_path_to_caption[path]\n",
        "  for caption in caption_list:\n",
        "    temp_caption_len = len(caption.split())\n",
        "    if (temp_caption_len < MIN_CAPTION_LEN) or (temp_caption_len > MAX_CAPTION_LEN):\n",
        "      caption_list.remove(caption)\n",
        "    else:\n",
        "      caption_lengths.append(temp_caption_len)\n",
        "  train_captions.append((path,caption_list))    "
      ],
      "metadata": {
        "id": "iITniox4HDmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Captions after filtering based on length:\",len(caption_lengths))\n",
        "\n",
        "n, bins, patches = plt.hist(caption_lengths, bins = (MAX_CAPTION_LEN - MIN_CAPTION_LEN + 1))\n",
        "plt.xlabel('Caption length')\n",
        "plt.ylabel('Number of captions')\n",
        "plt.title('Histogram of caption lengths')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "DcPCbhjbHFOY",
        "outputId": "1e94d9a4-1416-4ae3-f2e4-8d538b0f6ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions after filtering based on length: 102476\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Image feature extractor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δοκιμάσαμε αρκετά συνελικτικά για encoder μερικά απο τα οποία φαίνονται παρακάτω. Για το καλύτερο μας captioning επιλέξαμε τον VGG16."
      ],
      "metadata": {
        "id": "5RYJ1UE-fCOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "InceptionV3"
      ],
      "metadata": {
        "id": "wx_C-dZ-heRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(299, 299, 3)\n",
        "InceptionV3 = tf.keras.applications.InceptionV3(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "InceptionV3.trainable=False"
      ],
      "metadata": {
        "id": "El3i2uiEhgAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet121"
      ],
      "metadata": {
        "id": "VdLt2IvMhhNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "DenseNet121 = tf.keras.applications.DenseNet121(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "DenseNet121.trainable=False"
      ],
      "metadata": {
        "id": "rwld28BZhiI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50"
      ],
      "metadata": {
        "id": "CEqTt42Ihj2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "ResNet50 = tf.keras.applications.ResNet50(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "ResNet50.trainable=False"
      ],
      "metadata": {
        "id": "mNRjeAxXhk7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16"
      ],
      "metadata": {
        "id": "_Ffk153emD47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "VGG16 = tf.keras.applications.VGG16(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "VGG16.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(VGG16(test_img_batch).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BClwa3FXmDgp",
        "outputId": "ae84edda-ff32-4aae-bbe5-efaa3be42afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Setup the text tokenizer/vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για την συνάρτηση standardize επιλέξαμε να μην εισάγουμε άλλα φίλτρα καθώς παρατηρήσαμε πως τα captions δεν χρειάζονταν επιπλέον επεξεργασία αφού δεν βρήκαμε περιττή πληροφορία μέσα σε αυτά."
      ],
      "metadata": {
        "id": "Fyl7aeslhp6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NroZIzB90hD3"
      },
      "outputs": [],
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δοκιμάσαμε διάφορα vocabulary sizes μεταξύ 5000-10000. Παρατηρήσαμε πως το μέγεθος 8000 ήταν το ιδανικό."
      ],
      "metadata": {
        "id": "UvQUk-FchtCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9SQOXFsyS36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cadaf197-3f7a-4d12-d70a-65944b3cfb74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e9e04c5e28f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use the top 5000 words for a vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m tokenizer = tf.keras.layers.TextVectorization(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ],
      "source": [
        "# Use the top 5000(Default) words for a vocabulary.\n",
        "vocabulary_size = 8000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRahTDtWhJIf",
        "outputId": "6ed2e7a4-8a89-46f6-82e5-61f10f095c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'and', 'man']"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2mGxD33JCxN",
        "outputId": "ca8c71d4-5817-4851-d1f5-591eb690ea1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[3, 2, 755, 5, 2, 63, 4], [3, 2, 2866, 34, 4]]>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo-cfCX3LnHs",
        "outputId": "8e42efef-971f-40c1-da10-7e1e1b5512c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n",
              " [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrUUfGc65vAT",
        "outputId": "3184aa96-493f-4a8b-d54a-cbdc16e561c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "### Prepare the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aX0Z_98S2tN"
      },
      "source": [
        "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
        "\n",
        "This function will replicate the image so there are 1:1 images to captions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Lqwl9NiGT0"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZGUsuGzUfzt",
        "outputId": "107ef6ae-3713-465a-fa37-835cfda7960d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ENR_-swVhnm"
      },
      "source": [
        "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DsgQ_hZT4C2"
      },
      "outputs": [],
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA1x2j0JXX-N"
      },
      "source": [
        "This function adds operations to a dataset. The steps are:\n",
        "\n",
        "1. Load the images (and ignore images that fail to load).\n",
        "2. Replicate images to match the number of captions.\n",
        "3. Shuffle and rebatch the `image, caption` pairs.\n",
        "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
        "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_Pt9zldjQ0q"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrQ85t1GNfpQ"
      },
      "source": [
        "You could install the feature extractor in your model and train on the datasets like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KlhOG5cjQ0r",
        "outputId": "431f337e-62bd-46e3-f3bd-c30f4bfd8342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Zy9F3zX7i2",
        "outputId": "1a14cba0-e144-4c05-908a-db827566563e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI265LiDslr2"
      },
      "source": [
        "## Data ready for training\n",
        "\n",
        "After those preprocessing steps, here are the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B80JXj7HloX",
        "outputId": "80640cb6-e1ac-4fcb-b235-47c26804a541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jfb8qknlsKi"
      },
      "source": [
        "The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJBEwuXLZQdw",
        "outputId": "3af2e96f-a7e8-4efc-e704-6b3e1ff79be7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 224, 224, 3)\n",
            "(32, 26)\n",
            "(32, 26)\n"
          ]
        }
      ],
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22R58DzZoF17"
      },
      "source": [
        "The input tokens and the labels are the same, just shifted by 1 step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7h5UGftn1hT",
        "outputId": "99bc3a92-2153-4fce-c7e5-0a5678aec531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   3   13    5    2   22  101    8   48  128 2435   48  642    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
            "[  13    5    2   22  101    8   48  128 2435   48  642    4    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfICM49WFpIb"
      },
      "source": [
        "## A Transformer decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyjuWsmZoyO"
      },
      "source": [
        "This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n",
        "\n",
        "The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The Transformer encoder and decoder.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRXWwIKNybB"
      },
      "source": [
        "The model will be implemented in three main parts: \n",
        "\n",
        "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
        "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
        "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
        "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
        "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
        "1. Output - A multiclass-classification over the output vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngm3SQMCaYU"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9suaARZGPKw"
      },
      "source": [
        "The input text has already been split up into tokens and converted to sequences of IDs. \n",
        "\n",
        "Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n",
        "\n",
        "The `SeqEmbedding` layer defined below:\n",
        "\n",
        "- It looks up the embedding vector for each token.\n",
        "- It looks up an embedding vector for each sequence location.\n",
        "- It adds the two together.\n",
        "- It uses `mask_zero=True` to initialize the keras-masks for the model.\n",
        "\n",
        "Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P91LU2F0a9Ga"
      },
      "outputs": [],
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        mask_zero=True)\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With pre-trained embeddings"
      ],
      "metadata": {
        "id": "mkDJMuSumAvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDINGS\n",
        "#EMBEDDING_DIM = 256 # for False: default 256\n",
        "EMBEDDING_DIM = 300 # for True: 50, 100, 200 or 300\n",
        "\n",
        "VOCABULARY_SIZE = 10000 #default"
      ],
      "metadata": {
        "id": "FR5WnwywnTGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBxpmRIInTvN",
        "outputId": "7aeb12c7-dee1-4539-c813-0147a90fb9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-25 17:17:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-25 17:17:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-25 17:17:27--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 40s  \n",
            "\n",
            "2023-02-25 17:20:09 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove = os.path.join(os.path.expanduser(\"~\"), \"/content/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\")\n"
      ],
      "metadata": {
        "id": "t5rLl9renYiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, embedding_dim, path_to_glove_file):\n",
        "    super().__init__()\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(path_to_glove_file) as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "    num_tokens = vocab_size     \n",
        "\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "    words = tokenizer.get_vocabulary()\n",
        "    for i,word in enumerate(words):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "              \n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=embedding_dim)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "            mask_zero=True\n",
        "        )\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ],
      "metadata": {
        "id": "dSp8jfromATP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II1mD-bBCdMB"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHMLeMtKPTCW"
      },
      "source": [
        "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n",
        "\n",
        "The `CausalSelfAttention` layer is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c66OTRwQfd8"
      },
      "source": [
        "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIY6Vu2pLBAO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hn5p6f-RE0C"
      },
      "source": [
        "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWKrl7teOnH2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbXoiVNPRoJc"
      },
      "source": [
        "Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydcW5KZZHou7"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgbYrF5Csqu"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnKZkrklAQf"
      },
      "source": [
        "At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WQD87efena5"
      },
      "source": [
        "But there are a few other features you can add to make this work a little better:\n",
        "\n",
        "1. **Handle bad tokens**: The model will be generating text. It should\n",
        "   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n",
        "   `'[START]'`). So set the bias for these to a large negative value.\n",
        "\n",
        "   > Note: You'll need to ignore these tokens in the loss function as well. \n",
        "\n",
        "2. **Smart initialization**: The default initialization of a dense layer will\n",
        "  give a model that initially predicts each token with almost uniform\n",
        "  likelihood. The actual token distribution is far from uniform. The\n",
        "  optimal value for the initial bias of the output layer is the log of the\n",
        "  probability of each token. So include an `adapt` method to count the tokens\n",
        "  and set the optimal initial bias. This reduces the initial loss from the\n",
        "  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n",
        "  entropy of the distribution (`-p*log(p)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeWw2SFDHUfo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQHqANd1A6Q"
      },
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGnOQyc501B2",
        "outputId": "9f0b4d19-b432-4909-9f05-4f50a6f1b630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3282it [02:25, 22.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uniform entropy: 9.21\n",
            "Marginal entropy: 5.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gq-ICN7bD-u"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou4fPH_SWgH"
      },
      "source": [
        "To build the model, you need to combine several parts:\n",
        "\n",
        "1. The image `feature_extractor` and the text `tokenizer` and.\n",
        "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
        "   vectors `(batch, sequence, channels)`.\n",
        "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
        "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHCISYehH1f6"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer,embedding_dim,path_to_glove_file, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        max_length=max_length,\n",
        "        embedding_dim=embedding_dim,\n",
        "        path_to_glove_file=path_to_glove_file\n",
        "        )\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(embedding_dim, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW390dOz9T-x"
      },
      "source": [
        "When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n",
        "\n",
        "* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n",
        "* If the text has dtype `tf.string` run it through the tokenizer.\n",
        "\n",
        "After that running the model is only a few steps:\n",
        "\n",
        "1. Flatten the extracted image features, so they can be input to the decoder layers.\n",
        "2. Look up the token embeddings.\n",
        "3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n",
        "4. Run the output layer to predict the next token at each position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPdb7I4h9Ulo"
      },
      "outputs": [],
      "source": [
        "  @Captioner.add_method\n",
        "  def call(self, inputs):\n",
        "    image, txt = inputs\n",
        "\n",
        "    if image.shape[-1] == 3:\n",
        "      # Apply the feature-extractor, if you get an RGB image.\n",
        "      image = self.feature_extractor(image)\n",
        "    \n",
        "    # Flatten the feature map\n",
        "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "    if txt.dtype == tf.string:\n",
        "      # Apply the tokenizer if you get string inputs.\n",
        "      txt = tokenizer(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      txt = dec_layer(inputs=(image, txt))\n",
        "      \n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_VGG16 = Captioner(tokenizer, feature_extractor=VGG16, output_layer=output_layer,embedding_dim=100,path_to_glove_file=path_to_glove,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ],
      "metadata": {
        "id": "KzVNANH2H3bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmM7aZQsLiyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e8a53f17-0b0e-4adf-a3cb-2d36405e10ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-0b2596a3ba31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n\u001b[0m\u001b[1;32m      2\u001b[0m                   units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'embedding_dim' and 'path_to_glove_file'"
          ]
        }
      ],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "### Generate captions\n",
        "\n",
        "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
        "\n",
        "Start by downloading a test image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwFcdMqC-jE2"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRBIiTkubmxA"
      },
      "source": [
        "To caption an image with this model:\n",
        "\n",
        "- Extract the `img_features`\n",
        "- Initialize the list of output tokens with a `[START]` token.\n",
        "- Pass `img_features` and `tokens` into the model.\n",
        "  - It returns a list of logits.\n",
        "  - Choose the next token based on those logits.  \n",
        "  - Add it to the list of tokens, and continue the loop.\n",
        "  - If it generates an `'[END]'` token, break out of the loop.\n",
        "\n",
        "So add a \"simple\" method to do just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf1Jie9ef_Cg"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1): \n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPm96CccvHnq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "2455d1c0-5913-4104-a1b1-873ec0bf4535"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-924a46e7a8e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam Search Implementation"
      ],
      "metadata": {
        "id": "Vx8AAi2G-D_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def beam_search_gen(self, image, beam_size=3):\n",
        "    initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "    img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "    # Initialize beams\n",
        "    beams = [{'tokens': initial, 'score': 0.0}]\n",
        "    \n",
        "    for n in range(50):\n",
        "        new_beams = []\n",
        "        for beam in beams:\n",
        "            tokens = beam['tokens']\n",
        "            preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "            preds = preds[:,-1, :]  #(batch, vocab)\n",
        "            \n",
        "            # Get top beam_size predictions for each beam\n",
        "            top_preds_idx = tf.argsort(-preds, axis=1)[:, :beam_size]\n",
        "            \n",
        "            for i in range(beam_size):\n",
        "                next_token = top_preds_idx[0, i].numpy()\n",
        "                next_score = beam['score'] - tf.math.log(preds[0, next_token]).numpy()\n",
        "                new_tokens = tf.concat([tokens, [[next_token]]], axis=1)\n",
        "                new_beams.append({'tokens': new_tokens, 'score': next_score})\n",
        "                \n",
        "        # Keep the top beam_size beams\n",
        "        new_beams = sorted(new_beams, key=lambda x: x['score'])[:beam_size]\n",
        "        beams = new_beams\n",
        "        \n",
        "        # Check if all beams have ended\n",
        "        ended_beams = [beam for beam in beams if beam['tokens'][0, -1] == self.word_to_index('[END]')]\n",
        "        if len(ended_beams) == len(beams):\n",
        "            break\n",
        "    \n",
        "    # Choose the best beam\n",
        "    best_beam = sorted(beams, key=lambda x: x['score'])[0]\n",
        "    \n",
        "    words = index_to_word(best_beam['tokens'][0, 1:-1])\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "    \n",
        "    return result.numpy().decode()"
      ],
      "metadata": {
        "id": "4YkLSRcU-DNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model_VGG16.beam_search_gen(image, beam_size=3)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJiGY35DDaBo",
        "outputId": "c996e538-4108-4da6-90e7-82c3001c0bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxN2NPX2zB8y"
      },
      "source": [
        "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model_VGG16.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOcNIX9Bqpn",
        "outputId": "95941a92-ca22-4115-8535-a65b8d31c910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "a is a a and a a a\n",
            "a a twilight pinata a holding large a pedestrian wearing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JefwCRZ8z-Ah"
      },
      "source": [
        "The temperature parameter allows you to interpolate between 3 modes:\n",
        "\n",
        "1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n",
        "2. Random sampling according to the logits (`temperature=1.0`).\n",
        "3. Uniform random sampling (`temperature >> 1.0`). \n",
        "\n",
        "Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0FpTvaPkqON"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcwZdqObK-U"
      },
      "source": [
        "To train the model you'll need several additional components:\n",
        "\n",
        "- The Loss and metrics\n",
        "- The Optimizer\n",
        "- Optional Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IW2mWa2sAG"
      },
      "source": [
        "### Losses and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbpbDQTw1lOW"
      },
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s24im3FqxAfT"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhjHqgv3F2e"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dyQN9UfJYEd"
      },
      "source": [
        "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKDwbZOCZ-AP"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model_VGG16.beam_search_gen(self.image, beam_size=3)\n",
        "      print(result)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNA3_RAsdl0"
      },
      "source": [
        "It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGVLpzo13rcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8975f06c-c3f5-4a64-93dc-7c789c655122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "\n"
          ]
        }
      ],
      "source": [
        "g = GenerateText()\n",
        "g.model_VGG16 = model_VGG16\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxp4KZRKDk9"
      },
      "source": [
        "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjzrwGZp23xx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaJhQpcG8u0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBXG0dCDKO55"
      },
      "source": [
        "Configure and execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OR5ZpAII__u"
      },
      "outputs": [],
      "source": [
        "model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro955bQ2KR0X"
      },
      "source": [
        "For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n",
        "\n",
        "With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aB0baOVMZe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "699125e4-fe27-47ee-aaa5-28051ea5433e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-d7b165c1daba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history_VGG16 = model_VGG16.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             )\n\u001b[1;32m   1636\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1638\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 703\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    740\u001b[0m             self._flat_output_types)\n\u001b[1;32m    741\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3410\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P634LfVgw-eV"
      },
      "source": [
        "Plot the loss and accuracy over the training run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wn8KSkUw916"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZQ78b2Kxw-T"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQN1qT7KNqbL"
      },
      "source": [
        "## Attention plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XJaC2b2J23"
      },
      "source": [
        "Now, using the trained model,  run that `simple_gen` method on the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UQPtNTb2eu3"
      },
      "outputs": [],
      "source": [
        "result = model.simple_gen(image, temperature=0.0)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NXbmeLGN1bJ"
      },
      "source": [
        "Split the output back into tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHKOpm0w5Xto"
      },
      "outputs": [],
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE-AjuAV55Qo"
      },
      "source": [
        "The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZpyuQvq2q-B"
      },
      "outputs": [],
      "source": [
        "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "[map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42ImsWv6oHG"
      },
      "source": [
        "So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojwtvnkh6mS-"
      },
      "outputs": [],
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence (height width) -> sequence height width',\n",
        "    height=7, width=7,\n",
        "    reduction='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TM7rA3zGpJW"
      },
      "source": [
        "Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASWmWerGCZp3"
      },
      "outputs": [],
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv7XYGFUd-U7"
      },
      "source": [
        "So here is where the model was focusing attention while generating each token of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "    \n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI4NAAws9rvY"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTz0abQKMkV"
      },
      "source": [
        "Now put that together into a more usable function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mktpfW-SKQIJ"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "  \n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FntRkY11OiMw"
      },
      "outputs": [],
      "source": [
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rprk3HEvZuxb"
      },
      "source": [
        "## Try it on your own images\n",
        "\n",
        "For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Psd1quzaAWg"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "weights = (0.4, 0.3, 0.2, 0.1)\n",
        "smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "def sentence_bleu_calc(references,hyp):\n",
        "  return(nltk.translate.bleu_score.sentence_bleu(\n",
        "                                          references, \n",
        "                                          hyp, \n",
        "                                          weights = weights, \n",
        "                                          smoothing_function=smoothing_function))\n",
        "\n",
        "def corpus_bleu_calc(list_of_references, list_of_hypotheses):\n",
        "  return(nltk.translate.bleu_score.corpus_bleu(list_of_references, \n",
        "                                               list_of_hypotheses, \n",
        "                                               weights = weights, \n",
        "                                               smoothing_function=smoothing_function))"
      ],
      "metadata": {
        "id": "3SpVzOtCc2Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_references = [row[1] for row in test_captions]\n",
        "references = [[s.split()[:-1] for s in sublist] for sublist in image_references]\n",
        "image_paths = [row[0] for row in test_captions]"
      ],
      "metadata": {
        "id": "SlmI69IkdImA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_VGG19.simple_gen(myimage).split()\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "uq0A6ZjHdHih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder optimization"
      ],
      "metadata": {
        "id": "JTH4Mz7ocmfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RestNet152V2"
      ],
      "metadata": {
        "id": "gLjfvXhQkP1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "ResNet152V2 = tf.keras.applications.ResNet152V2(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "ResNet152V2.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_ResNet152V2 = train_ds\n",
        "test_ds_ResNet152V2 = test_ds\n",
        "#save_dataset(train_raw, 'ResNet152V2_train_cache', ResNet152V2, tokenizer)\n",
        "#save_dataset(test_raw, 'ResNet152V2_test_cache', ResNet152V2, tokenizer)\n",
        "\n",
        "#train_ds_ResNet152V2 = load_dataset('ResNet152V2_train_cache')\n",
        "#test_ds_ResNet152V2 = load_dataset('ResNet152V2_test_cache')\n",
        "\n",
        "model_ResNet152V2 = Captioner(tokenizer, feature_extractor=ResNet152V2, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_ResNet152V2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_ResNet152V2 = model_ResNet152V2.fit(\n",
        "    train_ds_ResNet152V2.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_ResNet152V2.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "0o1KoRu4ZHdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###VGG16"
      ],
      "metadata": {
        "id": "u9Ak7S6JvxHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "VGG16 = tf.keras.applications.VGG16(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "VGG16.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_VGG16 = train_ds\n",
        "test_ds_VGG16 = test_ds\n",
        "#save_dataset(train_raw, 'VGG16_train_cache', VGG16, tokenizer)\n",
        "#save_dataset(test_raw, 'VGG16_test_cache', VGG16, tokenizer)\n",
        "\n",
        "#train_ds_VGG16 = load_dataset('VGG16_train_cache')\n",
        "#test_ds_VGG16 = load_dataset('VGG16_test_cache')\n",
        "\n",
        "model_VGG16 = Captioner(tokenizer, feature_extractor=VGG16, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_VGG16 = model_VGG16.fit(\n",
        "    train_ds_VGG16.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_VGG16.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "BigwnpmwlzI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_VGG16.simple_gen(myimage).split()\n",
        "print(myhyp)\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "u6IHUITgct0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG19_hypothesis_captions = []\n",
        "\n",
        "for i_path in image_paths[:100]:\n",
        "  i_image = load_image(i_path)\n",
        "  i_hyp = model_VGG19.simple_gen(i_image).split()\n",
        "  VGG19_hypothesis_captions.append(i_hyp)"
      ],
      "metadata": {
        "id": "Sh0tzN-zcsaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu_calc(references[:100], VGG16_hypothesis_captions)"
      ],
      "metadata": {
        "id": "zC_FfIp_tpYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###InceptionV3"
      ],
      "metadata": {
        "id": "PnXhaKisv0DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(299, 299, 3)\n",
        "InceptionV3 = tf.keras.applications.InceptionV3(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "InceptionV3.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_InceptionV3 = train_ds\n",
        "test_ds_InceptionV3 = test_ds\n",
        "#save_dataset(train_raw, 'InceptionV3_train_cache', InceptionV3, tokenizer)\n",
        "#save_dataset(test_raw, 'InceptionV3_test_cache', InceptionV3, tokenizer)\n",
        "\n",
        "#train_ds_InceptionV3 = load_dataset('InceptionV3_train_cache')\n",
        "#test_ds_InceptionV3 = load_dataset('InceptionV3_test_cache')\n",
        "\n",
        "model_InceptionV3 = Captioner(tokenizer, feature_extractor=InceptionV3, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_InceptionV3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_InceptionV3 = model_InceptionV3.fit(\n",
        "    train_ds_InceptionV3.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_InceptionV3.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "qXkhC6KVv9LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ResNet50"
      ],
      "metadata": {
        "id": "1ncWplQjwztZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "ResNet50 = tf.keras.applications.ResNet50(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "ResNet50.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_ResNet50 = train_ds\n",
        "test_ds_ResNet50 = test_ds\n",
        "#save_dataset(train_raw, 'ResNet50_train_cache', ResNet50, tokenizer)\n",
        "#save_dataset(test_raw, 'ResNet50_test_cache', ResNet50, tokenizer)\n",
        "\n",
        "#train_ds_ResNet50 = load_dataset('ResNet50_train_cache')\n",
        "#test_ds_ResNet50 = load_dataset('ResNet50_test_cache')\n",
        "\n",
        "model_ResNet50 = Captioner(tokenizer, feature_extractor=ResNet50, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_ResNet50.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_ResNet50 = model_ResNet50.fit(\n",
        "    train_ds_ResNet50.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_ResNet50.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "DJtjQee7xWKa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "5756c6c9-7733-4ca4-ab1d-eaa025ee637e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-6b73cb2a8d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#test_ds_ResNet50 = load_dataset('ResNet50_test_cache')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m model_ResNet50 = Captioner(tokenizer, feature_extractor=ResNet50, output_layer=output_layer,\n\u001b[0m\u001b[1;32m     23\u001b[0m                   units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n\u001b[1;32m     24\u001b[0m model_ResNet50.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'embedding_dim' and 'path_to_glove_file'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_ResNet50.simple_gen(myimage).split()\n",
        "print(myhyp)\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "hFs8YL270dO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ResNet50_hypothesis_captions = []\n",
        "\n",
        "for i_path in image_paths[:100]:\n",
        "  i_image = load_image(i_path)\n",
        "  i_hyp = model_ResNet50.simple_gen(i_image).split()\n",
        "  ResNet50_hypothesis_captions.append(i_hyp)"
      ],
      "metadata": {
        "id": "vDjgFf270m5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu_calc(references[:100], ResNet50_hypothesis_captions)"
      ],
      "metadata": {
        "id": "vZWDYGEe0pex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DenseNet121"
      ],
      "metadata": {
        "id": "BEucB8xG6bAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "DenseNet121 = tf.keras.applications.DenseNet121(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "DenseNet121.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_DenseNet121 = train_ds\n",
        "test_ds_DenseNet121 = test_ds\n",
        "#save_dataset(train_raw, 'DenseNet121_train_cache', DenseNet121, tokenizer)\n",
        "#save_dataset(test_raw, 'DenseNet121_test_cache', DenseNet121, tokenizer)\n",
        "\n",
        "#train_ds_DenseNet121 = load_dataset('DenseNet121_train_cache')\n",
        "#test_ds_DenseNet121 = load_dataset('DenseNet121_test_cache')\n",
        "\n",
        "model_DenseNet121 = Captioner(tokenizer, feature_extractor=DenseNet121, output_layer=output_layer,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_DenseNet121.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_DenseNet121 = model_DenseNet121.fit(\n",
        "    train_ds_DenseNet121.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_DenseNet121.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "KIsfFhFw6mA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_DenseNet121.simple_gen(myimage).split()\n",
        "print(myhyp)\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "I6OWRGVn7P53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DenseNet121_hypothesis_captions = []\n",
        "\n",
        "for i_path in image_paths[:100]:\n",
        "  i_image = load_image(i_path)\n",
        "  i_hyp = model_DenseNet121.simple_gen(i_image).split()\n",
        "  DenseNet121_hypothesis_captions.append(i_hyp)"
      ],
      "metadata": {
        "id": "S133_nwP7WD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu_calc(references[:100], DenseNet121_hypothesis_captions)"
      ],
      "metadata": {
        "id": "8AJGBYKS7atF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embeddings"
      ],
      "metadata": {
        "id": "u15iab5LBwsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "VGG16 = tf.keras.applications.VGG16(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "VGG16.trainable=False\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "train_ds_VGG16 = train_ds\n",
        "test_ds_VGG16 = test_ds\n",
        "#save_dataset(train_raw, 'VGG16_train_cache', VGG16, tokenizer)\n",
        "#save_dataset(test_raw, 'VGG16_test_cache', VGG16, tokenizer)\n",
        "\n",
        "#train_ds_VGG16 = load_dataset('VGG16_train_cache')\n",
        "#test_ds_VGG16 = load_dataset('VGG16_test_cache')\n",
        "\n",
        "model_VGG16 = Captioner(tokenizer, feature_extractor=VGG16, output_layer=output_layer,embedding_dim=100,path_to_glove_file=path_to_glove,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)\n",
        "model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])\n",
        "history_VGG16 = model_VGG16.fit(\n",
        "    train_ds_VGG16.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds_VGG16.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "jTMTQct0rjC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c0a02ca-e9c9-403b-e733-16ac2039a1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.5157 - masked_acc: 0.1419\n",
            "\n",
            "a\n",
            "a in a a girl is with a in a\n",
            "a these heart for in toward a large\n",
            "\n",
            "100/100 [==============================] - 46s 313ms/step - loss: 5.5157 - masked_acc: 0.1419 - val_loss: 5.3717 - val_masked_acc: 0.1778\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.3122 - masked_acc: 0.1886\n",
            "\n",
            "a man man man man man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in\n",
            "a man in a man in the in a man playing\n",
            "wearing band herself it tugofwar\n",
            "\n",
            "100/100 [==============================] - 30s 304ms/step - loss: 5.3122 - masked_acc: 0.1886 - val_loss: 5.1885 - val_masked_acc: 0.2011\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.1493 - masked_acc: 0.2109\n",
            "\n",
            "a man in a man in a man\n",
            "a looks in a with the are is on the is\n",
            "a is a spread a rifle square boy forehead shirt on a tightrope leather in a boy sits in a rock and pouring sunlight crowd and bicycle to presentation with a background\n",
            "\n",
            "100/100 [==============================] - 29s 288ms/step - loss: 5.1493 - masked_acc: 0.2109 - val_loss: 5.0630 - val_masked_acc: 0.2163\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.0849 - masked_acc: 0.2136\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a white\n",
            "a caught well their woman hat dog\n",
            "\n",
            "100/100 [==============================] - 29s 295ms/step - loss: 5.0849 - masked_acc: 0.2136 - val_loss: 4.9305 - val_masked_acc: 0.2254\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.9678 - masked_acc: 0.2285\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a man a man in a young orange is\n",
            "all with on a blue man sitting with the bike\n",
            "\n",
            "100/100 [==============================] - 29s 295ms/step - loss: 4.9678 - masked_acc: 0.2285 - val_loss: 4.8499 - val_masked_acc: 0.2361\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.9520 - masked_acc: 0.2268\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a man playing in a woman\n",
            "an woman and musicians area in black old mask trumpet\n",
            "\n",
            "100/100 [==============================] - 29s 294ms/step - loss: 4.9520 - masked_acc: 0.2268 - val_loss: 4.8086 - val_masked_acc: 0.2362\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.8828 - masked_acc: 0.2350\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a man is top on a group\n",
            "a group of the ball\n",
            "\n",
            "100/100 [==============================] - 29s 290ms/step - loss: 4.8828 - masked_acc: 0.2350 - val_loss: 4.8137 - val_masked_acc: 0.2411\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.8000 - masked_acc: 0.2398\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a woman on a in a man is in the man is is is with a white dog\n",
            "along waves\n",
            "\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 4.8000 - masked_acc: 0.2398 - val_loss: 4.7282 - val_masked_acc: 0.2457\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7454 - masked_acc: 0.2444\n",
            "\n",
            "a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a man in a\n",
            "a man is in a\n",
            "four young a blue rodeo drum and league in wooded smile\n",
            "\n",
            "100/100 [==============================] - 29s 293ms/step - loss: 4.7454 - masked_acc: 0.2444 - val_loss: 4.6918 - val_masked_acc: 0.2582\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7417 - masked_acc: 0.2510\n",
            "\n",
            "a man is in a dog is in a dog\n",
            "a young boy is walking on the edge\n",
            "a boys boy and a boy is rides and bee\n",
            "\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 4.7417 - masked_acc: 0.2510 - val_loss: 4.5170 - val_masked_acc: 0.2670\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6250 - masked_acc: 0.2595\n",
            "\n",
            "a man is in a black dog in a black dog\n",
            "a young man in a black shirt is standing in a black dog\n",
            "plaid dog a striped arrangements homemade a cookie\n",
            "\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 4.6250 - masked_acc: 0.2595 - val_loss: 4.6241 - val_masked_acc: 0.2537\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6041 - masked_acc: 0.2575\n",
            "\n",
            "a man is in a dog in a dog in a dog\n",
            "a man is is in a boy and a dog in a little tree in a white and a man is\n",
            "a hair in a ground her meat an field other many grass\n",
            "\n",
            "100/100 [==============================] - 29s 291ms/step - loss: 4.6041 - masked_acc: 0.2575 - val_loss: 4.5584 - val_masked_acc: 0.2668\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5745 - masked_acc: 0.2608\n",
            "\n",
            "a man is in a black shirt is in a red shirt is in a dog\n",
            "a girl with a water\n",
            "a man with a white stand in yellow home dog take in in front of a motorcycles\n",
            "\n",
            "100/100 [==============================] - 26s 261ms/step - loss: 4.5745 - masked_acc: 0.2608 - val_loss: 4.5124 - val_masked_acc: 0.2654\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5561 - masked_acc: 0.2648\n",
            "\n",
            "a man is in a black shirt is in the water\n",
            "a man is is walking on a dog\n",
            "this scary little and a a group\n",
            "\n",
            "100/100 [==============================] - 27s 266ms/step - loss: 4.5561 - masked_acc: 0.2648 - val_loss: 4.4041 - val_masked_acc: 0.2650\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5141 - masked_acc: 0.2652\n",
            "\n",
            "a man is in a red shirt is standing in the water\n",
            "a girl is playing in the white shirt and a large small young hair is in the the water of a bench\n",
            "the action\n",
            "\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 4.5141 - masked_acc: 0.2652 - val_loss: 4.4092 - val_masked_acc: 0.2763\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5172 - masked_acc: 0.2659\n",
            "\n",
            "a man is in a black shirt is in a red shirt is in a red shirt is in a dog\n",
            "a man is walking in a bicycle\n",
            "an\n",
            "\n",
            "100/100 [==============================] - 26s 261ms/step - loss: 4.5172 - masked_acc: 0.2659 - val_loss: 4.3188 - val_masked_acc: 0.2753\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4468 - masked_acc: 0.2729\n",
            "\n",
            "a man in a black shirt is in a black shirt is in the water\n",
            "a boy is walking on a bike\n",
            "two men in the ends in dark red water\n",
            "\n",
            "100/100 [==============================] - 26s 260ms/step - loss: 4.4468 - masked_acc: 0.2729 - val_loss: 4.3076 - val_masked_acc: 0.2809\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4475 - masked_acc: 0.2688\n",
            "\n",
            "a man is is in a blue shirt is in a blue shirt is in a blue shirt is in the water\n",
            "a a man is walking in the water\n",
            "a boy is getting skatepark posed towards the bike each ocean\n",
            "\n",
            "100/100 [==============================] - 26s 261ms/step - loss: 4.4475 - masked_acc: 0.2688 - val_loss: 4.3488 - val_masked_acc: 0.2748\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3939 - masked_acc: 0.2722\n",
            "\n",
            "a man is in a blue shirt is in a blue shirt is in a beach\n",
            "a man is riding a grass\n",
            "an other a boy\n",
            "\n",
            "100/100 [==============================] - 25s 253ms/step - loss: 4.3939 - masked_acc: 0.2722 - val_loss: 4.3092 - val_masked_acc: 0.2799\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4260 - masked_acc: 0.2704\n",
            "\n",
            "a man in a blue shirt is standing on a beach\n",
            "a man in a a pink is walking on a beach\n",
            "a older lady shorts and watches on a baby or with a ball in a small view full large day on with a brown ball of his left while chubby boat water\n",
            "\n",
            "100/100 [==============================] - 28s 277ms/step - loss: 4.4260 - masked_acc: 0.2704 - val_loss: 4.2450 - val_masked_acc: 0.2854\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3886 - masked_acc: 0.2786\n",
            "\n",
            "a man is in a blue shirt is standing in a beach\n",
            "a girl is running on a water\n",
            "two dogs are people are swings on in water rock\n",
            "\n",
            "100/100 [==============================] - 27s 268ms/step - loss: 4.3886 - masked_acc: 0.2786 - val_loss: 4.2455 - val_masked_acc: 0.2781\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3614 - masked_acc: 0.2806\n",
            "\n",
            "a man in a black and a black and a black and a black and a black and a black and a black and a black and a black and a black dog is walking in the water\n",
            "a man is riding a dog\n",
            "two mud bucks tracks the bmx after side of a fence\n",
            "\n",
            "100/100 [==============================] - 28s 280ms/step - loss: 4.3614 - masked_acc: 0.2806 - val_loss: 4.1722 - val_masked_acc: 0.2865\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3121 - masked_acc: 0.2829\n",
            "\n",
            "a man is in a blue shirt is standing in the water\n",
            "a man is is standing on the water\n",
            "a tennis share dj him through the flowers\n",
            "\n",
            "100/100 [==============================] - 26s 259ms/step - loss: 4.3121 - masked_acc: 0.2829 - val_loss: 4.1735 - val_masked_acc: 0.2888\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2800 - masked_acc: 0.2870\n",
            "\n",
            "a man is standing in a water\n",
            "a man is taking a stick on a water\n",
            "brown is smiles on the background\n",
            "\n",
            "100/100 [==============================] - 26s 257ms/step - loss: 4.2800 - masked_acc: 0.2870 - val_loss: 4.1662 - val_masked_acc: 0.2829\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2927 - masked_acc: 0.2828\n",
            "\n",
            "a man is running in the water\n",
            "a white dog is running in a field\n",
            "a black boy dressed in a motorcycle in the grass\n",
            "\n",
            "100/100 [==============================] - 26s 257ms/step - loss: 4.2927 - masked_acc: 0.2828 - val_loss: 4.1252 - val_masked_acc: 0.2907\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2630 - masked_acc: 0.2824\n",
            "\n",
            "a man is jumping in the water\n",
            "a man is playing in the water\n",
            "two a boy hanging at snow air\n",
            "\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 4.2630 - masked_acc: 0.2824 - val_loss: 4.1679 - val_masked_acc: 0.2927\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2699 - masked_acc: 0.2852\n",
            "\n",
            "a man is running in the water\n",
            "a girl in a blue shirt is standing in a black water\n",
            "icy boy cooking on the mountain\n",
            "\n",
            "100/100 [==============================] - 25s 246ms/step - loss: 4.2699 - masked_acc: 0.2852 - val_loss: 4.1325 - val_masked_acc: 0.2913\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2340 - masked_acc: 0.2873\n",
            "\n",
            "a man is riding a water\n",
            "a boy is running on a water\n",
            "boy and black swing along water with a cars one the sunny busy chased\n",
            "\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 4.2340 - masked_acc: 0.2873 - val_loss: 4.1011 - val_masked_acc: 0.2920\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2331 - masked_acc: 0.2891\n",
            "\n",
            "a man in a blue shirt is running in a water\n",
            "a person in a black shirt is is riding a field in a water\n",
            "a child tshirt\n",
            "\n",
            "100/100 [==============================] - 26s 257ms/step - loss: 4.2331 - masked_acc: 0.2891 - val_loss: 4.0608 - val_masked_acc: 0.2984\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2343 - masked_acc: 0.2870\n",
            "\n",
            "a man is riding a water\n",
            "a girl is riding a water with a pool\n",
            "two pouring side other\n",
            "\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 4.2343 - masked_acc: 0.2870 - val_loss: 4.1740 - val_masked_acc: 0.2966\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2520 - masked_acc: 0.2883\n",
            "\n",
            "a man in a blue shirt is riding a water\n",
            "a young boy in the water\n",
            "a group of a little boy wearing pink using a blue is holding from the water\n",
            "\n",
            "100/100 [==============================] - 27s 271ms/step - loss: 4.2520 - masked_acc: 0.2883 - val_loss: 4.1477 - val_masked_acc: 0.2797\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1940 - masked_acc: 0.2923\n",
            "\n",
            "a man is running in the water\n",
            "a man is riding a swimming in a beach\n",
            "a little boy in a piece of snow\n",
            "\n",
            "100/100 [==============================] - 26s 259ms/step - loss: 4.1940 - masked_acc: 0.2923 - val_loss: 4.0910 - val_masked_acc: 0.2944\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1513 - masked_acc: 0.2954\n",
            "\n",
            "a man is riding a water\n",
            "a man is running through the water\n",
            "man in black jumps and school is standing near\n",
            "\n",
            "100/100 [==============================] - 29s 293ms/step - loss: 4.1513 - masked_acc: 0.2954 - val_loss: 4.0484 - val_masked_acc: 0.3043\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1354 - masked_acc: 0.2956\n",
            "\n",
            "a man is running in the water\n",
            "a man is running a pool\n",
            "a group of red changes bounce\n",
            "\n",
            "100/100 [==============================] - 31s 309ms/step - loss: 4.1354 - masked_acc: 0.2956 - val_loss: 4.0340 - val_masked_acc: 0.3064\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1106 - masked_acc: 0.2980\n",
            "\n",
            "a boy is running in the water\n",
            "a young boy is playing in a pool\n",
            "brown suit dangerous in a white shirt stands in a ocean ride in the beach\n",
            "\n",
            "100/100 [==============================] - 26s 258ms/step - loss: 4.1106 - masked_acc: 0.2980 - val_loss: 4.1033 - val_masked_acc: 0.2930\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0657 - masked_acc: 0.2974\n",
            "\n",
            "a man in a blue shirt is running in the water\n",
            "a man wearing a blue shirt is running through a pool\n",
            "a girl swings up blue next to a surfer wearing blue grass near a woman playing a pool\n",
            "\n",
            "100/100 [==============================] - 27s 275ms/step - loss: 4.0657 - masked_acc: 0.2974 - val_loss: 4.0766 - val_masked_acc: 0.2970\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0580 - masked_acc: 0.3041\n",
            "\n",
            "a man in a red shirt is running in the water\n",
            "a girl in a white shirt riding a air\n",
            "yellow little boy operating jogging in a large pool inside as water\n",
            "\n",
            "100/100 [==============================] - 26s 261ms/step - loss: 4.0580 - masked_acc: 0.3041 - val_loss: 4.0585 - val_masked_acc: 0.3013\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0685 - masked_acc: 0.2985\n",
            "\n",
            "a man is running in the water\n",
            "a child is jumping the water\n",
            "a girl doing a log watching a middle of a golden blue pool\n",
            "\n",
            "100/100 [==============================] - 26s 257ms/step - loss: 4.0685 - masked_acc: 0.2985 - val_loss: 4.0655 - val_masked_acc: 0.2984\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0892 - masked_acc: 0.2966\n",
            "\n",
            "a man in a blue shirt is jumping in the water\n",
            "a boy in the water\n",
            "a skinny rifle top upward over ocean in a retriever covered discussion tree\n",
            "\n",
            "100/100 [==============================] - 26s 262ms/step - loss: 4.0892 - masked_acc: 0.2966 - val_loss: 4.0368 - val_masked_acc: 0.2998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_VGG16 = model_VGG16.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ze8wvogGupY",
        "outputId": "319ca054-bb68-4081-ec98-9c2020036338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_VGG16.simple_gen(myimage).split()\n",
        "print(myhyp)\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "MQ-dT-odyEob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "559c512c-6540-4ed3-933f-642bd00b1157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['five', 'dogs', 'with', 'black', 'holding', 'establishment', 'his', 'boots', 'coat', 'in', 'a', 'cityscape', 'park', 'do', 'holding', 'into', 'a', 'lab', 'while', 'a', 'little', 'pole']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05489335497418165"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_hypothesis_captions = []\n",
        "\n",
        "for i_path in image_paths[:150]:\n",
        "  i_image = load_image(i_path)\n",
        "  i_hyp = model_VGG16.simple_gen(i_image).split()\n",
        "  VGG16_hypothesis_captions.append(i_hyp)"
      ],
      "metadata": {
        "id": "Djs8PQSlx6wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu_calc(references[:150], VGG16_hypothesis_captions)"
      ],
      "metadata": {
        "id": "dhR731cGx8NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48ed61f-7eee-411d-b719-4aea0e845ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05851768275813604"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image captioning with visual attention"
      ],
      "metadata": {
        "id": "sCFxeFNDLIPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ομάδα 57:\n",
        "\n",
        "Ανδρέας Χατζησάββας 03118701\n",
        "\n",
        "Θεόδωρος Σωτήρου 03118209\n",
        "\n",
        "Λουκία Παυλανά 03118711"
      ],
      "metadata": {
        "id": "ecSRkKMSR1FQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "wCUYZzdrR66O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyYffABMLMAI",
        "outputId": "5928b95f-6ad3-4325-8270-8f1bb3fc8ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcudnn8 is already the newest version (8.1.0.77-1+cuda11.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 17 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jVke4fSLMq-",
        "outputId": "2cf295d0-9f34-4e40-8987-9ab30c429e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.11.0\n",
            "Uninstalling tensorflow-2.11.0:\n",
            "  Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Successfully uninstalled keras-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDFOYYjrLQ9_",
        "outputId": "ab3b9704-db54-4f40-a808-443a29e42c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.8/dist-packages (4.8.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (4.64.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (5.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (7.1.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (5.4.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (2.25.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.14.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Installing collected packages: keras, tensorflow\n",
            "Successfully installed keras-2.11.0 tensorflow-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbg1W86mLSk_",
        "outputId": "faeca791-98e4-468e-82f9-839cf8f512c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "0uuUMqkdSDVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "sXjJiKitLUFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the dataset"
      ],
      "metadata": {
        "id": "JUkiCSn4SGOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download image files\n",
        "image_zip = tf.keras.utils.get_file('flickr30k-images-ecemod.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip',\n",
        "                                      extract=True)\n",
        "os.remove(image_zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EakKhOgdLVu_",
        "outputId": "dea3d176-4844-4330-e0eb-323c196bed11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip\n",
            "4376381805/4376381805 [==============================] - 440s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download captions file\n",
        "captions_file = tf.keras.utils.get_file('captions_new.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/captions_new.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download train files list\n",
        "train_files_list = tf.keras.utils.get_file('train_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/train_files.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download test files list\n",
        "test_files_list = tf.keras.utils.get_file('test_files.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/test_files.csv',\n",
        "                                           extract=False)"
      ],
      "metadata": {
        "id": "zpndSYTQLXNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\".\"\n",
        "IMAGE_DIR=\"image_dir\"\n",
        "path = pathlib.Path(path)\n",
        "   \n",
        "captions = (path/captions_file).read_text().splitlines()\n",
        "captions = (line.split('\\t') for line in captions)\n",
        "captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "   \n",
        "cap_dict = collections.defaultdict(list)\n",
        "for fname, cap in captions:\n",
        "  cap_dict[fname].append(cap)\n",
        "   \n",
        "train_files = (path/train_files_list).read_text().splitlines()\n",
        "train_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in train_files]\n",
        "   \n",
        "test_files = (path/test_files_list).read_text().splitlines()\n",
        "test_captions = [(str(path/IMAGE_DIR/fname), cap_dict[fname]) for fname in test_files]\n",
        "   \n",
        "train_raw = tf.data.experimental.from_list(train_captions)\n",
        "test_raw = tf.data.experimental.from_list(test_captions)"
      ],
      "metadata": {
        "id": "zw3n6zziLYQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQybV3AELaNT",
        "outputId": "29a73ee1-00a4-4e3b-b8fd-96da1178cdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDn34geLdl4",
        "outputId": "bafafba9-6c26-473e-e50a-51ca3de706e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'image_dir/_3430497.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'The skier is wearing a yellow jumpsuit and sliding across a yellow rail .'\n",
            " b'A yellow uniformed skier is performing a trick across a railed object .'\n",
            " b'A skier in electric green on the edge of a ramp made of metal bars .'\n",
            " b'A person on skis on a rail at night .'\n",
            " b'A skier slides along a metal rail .'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "is8PGv-NSOx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10000 #5000,6000,7000,8000,9000,10000\n",
        "VOCABULARY_SIZE = 10000\n",
        "\n",
        "#HYPERPARAMETERS\n",
        "MIN_CAPTION_LEN = 4\n",
        "MAX_CAPTION_LEN = 30\n",
        "\n",
        "#EMBEDDINGS\n",
        "#EMBEDDING_DIM = 256 # for False: default 256\n",
        "EMBEDDING_DIM = 300 # for True: 50, 100, 200 or 300"
      ],
      "metadata": {
        "id": "C0-fvz3LSQFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data"
      ],
      "metadata": {
        "id": "iJimByDfSSI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_to_caption = collections.defaultdict(list)\n",
        "\n",
        "for path, captions in train_captions:\n",
        "    for caption in captions:\n",
        "        image_path_to_caption[path].append(caption)\n",
        "print(image_path_to_caption['image_dir/_3430497.jpg'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9t37AFSLekY",
        "outputId": "5eb4c5f3-2ec1-4628-a253-94702eee0297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The skier is wearing a yellow jumpsuit and sliding across a yellow rail .', 'A yellow uniformed skier is performing a trick across a railed object .', 'A skier in electric green on the edge of a ramp made of metal bars .', 'A person on skis on a rail at night .', 'A skier slides along a metal rail .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list with all the paths of the train captions\n",
        "image_paths = [row[0] for row in train_captions]"
      ],
      "metadata": {
        "id": "QU8giy42LgBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ένα \"καλό\" παράθυρο μήκους για τα captions αποφασίσαμε στο [4,30]."
      ],
      "metadata": {
        "id": "ZfwctTM5iRoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_lengths = []\n",
        "train_captions = []\n",
        "\n",
        "for path in image_paths:\n",
        "  caption_list = image_path_to_caption[path]\n",
        "  for caption in caption_list:\n",
        "    temp_caption_len = len(caption.split())\n",
        "    if (temp_caption_len < MIN_CAPTION_LEN) or (temp_caption_len > MAX_CAPTION_LEN):\n",
        "      caption_list.remove(caption)\n",
        "    else:\n",
        "      caption_lengths.append(temp_caption_len)\n",
        "  train_captions.append((path,caption_list))    "
      ],
      "metadata": {
        "id": "Iz4dFP3qLg7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Captions after filtering based on length:\",len(caption_lengths))\n",
        "\n",
        "n, bins, patches = plt.hist(caption_lengths, bins = (MAX_CAPTION_LEN - MIN_CAPTION_LEN + 1))\n",
        "plt.xlabel('Caption length')\n",
        "plt.ylabel('Number of captions')\n",
        "plt.title('Histogram of caption lengths')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "_4Wij91OLiBS",
        "outputId": "7990f164-63df-43da-af77-8eb39ffd35bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions after filtering based on length: 102476\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image feature extractor\n"
      ],
      "metadata": {
        "id": "rDUCBbatSYOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δοκιμάσαμε αρκετά συνελικτικά για encoder μερικά απο τα οποία φαίνονται παρακάτω. Για το καλύτερο μας captioning επιλέξαμε τον VGG16."
      ],
      "metadata": {
        "id": "4lxa3bzJiUQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "InceptionV3"
      ],
      "metadata": {
        "id": "8PWSu9_eiUyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(299, 299, 3)\n",
        "InceptionV3 = tf.keras.applications.InceptionV3(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "InceptionV3.trainable=False"
      ],
      "metadata": {
        "id": "zvY1CJbOiWiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet121"
      ],
      "metadata": {
        "id": "UqFHd5ktiXX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "DenseNet121 = tf.keras.applications.DenseNet121(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "DenseNet121.trainable=False"
      ],
      "metadata": {
        "id": "gt6KI2xkiY41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50"
      ],
      "metadata": {
        "id": "9GpffJxZiZxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "ResNet50 = tf.keras.applications.ResNet50(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "ResNet50.trainable=False"
      ],
      "metadata": {
        "id": "Ff7SZc8AibD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VGG16"
      ],
      "metadata": {
        "id": "YxI5v6kjieI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "VGG16 = tf.keras.applications.VGG16(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "VGG16.trainable=False"
      ],
      "metadata": {
        "id": "A-Qt_V5iLjp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ],
      "metadata": {
        "id": "5TellnhKLk_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(VGG16(test_img_batch).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgmqDIGWLlrn",
        "outputId": "ec70a513-a78e-4068-c224-85c4e72c3782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the text tokenizer/vectorizer"
      ],
      "metadata": {
        "id": "5rUG22kASeKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Για την συνάρτηση standardize επιλέξαμε να μην εισάγουμε άλλα φίλτρα καθώς παρατηρήσαμε πως τα captions δεν χρειάζονταν επιπλέον επεξεργασία αφού δεν βρήκαμε περιττή πληροφορία μέσα σε αυτά."
      ],
      "metadata": {
        "id": "q5H9yinziiha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s"
      ],
      "metadata": {
        "id": "0t4hoLcnLmiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δοκιμάσαμε διάφορα vocabulary sizes μεταξύ 5000-10000. Παρατηρήσαμε πως το μέγεθος 8000 ήταν το ιδανικό."
      ],
      "metadata": {
        "id": "djvyat0Nikla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the top 5000(Default) words for a vocabulary.\n",
        "vocabulary_size = 8000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ],
      "metadata": {
        "id": "pfGMnM52Lnjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6YP3yqxLodo",
        "outputId": "dacd33b6-3365-4bba-e223-40f180b06fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ],
      "metadata": {
        "id": "L5_L8YvlLp6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the datasets"
      ],
      "metadata": {
        "id": "cI_Q0i9wSkqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n",
        "\n",
        "This function will replicate the image so there are 1:1 images to captions:"
      ],
      "metadata": {
        "id": "0_L6O2PwSn2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ],
      "metadata": {
        "id": "MyYy5_KNLtw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7rAx__TLu1n",
        "outputId": "55816102-ce10-4edb-bec6-203b036cfa03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"
      ],
      "metadata": {
        "id": "1CUW-oUzSspL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ],
      "metadata": {
        "id": "vPXjEfb4Lv8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function adds operations to a dataset. The steps are:\n",
        "\n",
        "1. Load the images (and ignore images that fail to load).\n",
        "2. Replicate images to match the number of captions.\n",
        "3. Shuffle and rebatch the `image, caption` pairs.\n",
        "4. Tokenize the text, shift the tokens and add `label_tokens`.\n",
        "5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."
      ],
      "metadata": {
        "id": "E9hMIKowSukn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ],
      "metadata": {
        "id": "PkSIj3zyLw23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xQYcKU3LxuI",
        "outputId": "46a210ed-beca-421d-c9be-c0ec902a0fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-03f5d7fa769a>:6: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.ignore_errors` instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Td2oprjLymY",
        "outputId": "6324c731-35f8-45fd-b13d-5c12369cfc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data ready for training\n",
        "\n",
        "After those preprocessing steps, here are the datasets:"
      ],
      "metadata": {
        "id": "aqZM5eS1S3sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6LGCYiCLzPY",
        "outputId": "286e6964-8f5f-4d08-e51c-ec8058859ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 224, 224, 3)\n",
            "(32, 33)\n",
            "(32, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5KQozx9Lz_H",
        "outputId": "1a5b70f3-98a2-461c-af74-d76321c3a509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  3   2   9   5   2  27  23  10 171 250   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[  2   9   5   2  27  23  10 171 250   4   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Transformer decoder model"
      ],
      "metadata": {
        "id": "EJGkWJA2TNRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will be implemented in three main parts: \n",
        "\n",
        "1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n",
        "1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n",
        "   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n",
        "   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n",
        "   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n",
        "1. Output - A multiclass-classification over the output vocabulary.\n"
      ],
      "metadata": {
        "id": "09WBVvQ2TOIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input with Pre-trained Embeddings of size 300"
      ],
      "metadata": {
        "id": "NP1BErG5TRV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αντί embeddings που μαθαίνονται κατά την εκπαίδευση του μοντέλου προτιμήσαμε έτοιμα embeddings με μεταφορά μάθησης. Το δίκτυο απέδιδε καλύτερα με embeddings διαστάσεων 300."
      ],
      "metadata": {
        "id": "wnMAjQqIiykK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jqbZ655L1xg",
        "outputId": "025b075d-a16e-407b-cfe9-019939ea7c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-26 18:00:47--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-02-26 18:00:47--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-02-26 18:00:48--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2023-02-26 18:03:28 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove = os.path.join(os.path.expanduser(\"~\"), \"/content/glove.6B.\" + str(EMBEDDING_DIM) + \"d.txt\")"
      ],
      "metadata": {
        "id": "93eUc5okL3C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τροποποιήσαμε την κλάση SeqEmbedding ώστε να εισάγει τα pre-trained Embeddings."
      ],
      "metadata": {
        "id": "WsgD2UvujThq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, embedding_dim, path_to_glove_file):\n",
        "    super().__init__()\n",
        "\n",
        "    embeddings_index = {}\n",
        "    with open(path_to_glove_file) as f:\n",
        "        for line in f:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embeddings_index[word] = coefs\n",
        "    num_tokens = vocab_size     \n",
        "\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "    words = tokenizer.get_vocabulary()\n",
        "    for i,word in enumerate(words):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "              \n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=embedding_dim)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "            trainable=False,\n",
        "            mask_zero=True\n",
        "        )\n",
        "    \n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    return self.add([seq,x])"
      ],
      "metadata": {
        "id": "25uVga0yL4YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "O-lfG1XWTYVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer).\n",
        "\n",
        "The `CausalSelfAttention` layer is below:"
      ],
      "metadata": {
        "id": "X05iAfkYTaMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "NbRZwYnnL8vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `CrossAttention` layer is below. Note the use of `return_attention_scores`."
      ],
      "metadata": {
        "id": "9qlk7gP8Tiam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "upwhZObHL922"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "
      ],
      "metadata": {
        "id": "42G0mDajTjv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "uBRwwZJKL-y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "IwCbJRL3L_yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "MpPNHDp0TsRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x + self.bias"
      ],
      "metadata": {
        "id": "lkkPLaSXMBm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ],
      "metadata": {
        "id": "agVi2tLiTul_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ndijGNpMDFK",
        "outputId": "3f5393a5-d143-4580-c25e-6868fdb1e63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3282it [02:54, 18.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Uniform entropy: 8.99\n",
            "Marginal entropy: 5.55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the model"
      ],
      "metadata": {
        "id": "Q_qGAKybTyin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Κάναμε τις απαραίτητες αλλαγές στον Captioner σύμφωνα με τις αλλαγές που κάναμε."
      ],
      "metadata": {
        "id": "qWE77mquTz52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer,embedding_dim,path_to_glove_file, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        max_length=max_length,\n",
        "        embedding_dim=embedding_dim,\n",
        "        path_to_glove_file=path_to_glove_file\n",
        "        )\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(embedding_dim, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ],
      "metadata": {
        "id": "6TYLt007MEFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  @Captioner.add_method\n",
        "  def call(self, inputs):\n",
        "    image, txt = inputs\n",
        "\n",
        "    if image.shape[-1] == 3:\n",
        "      # Apply the feature-extractor, if you get an RGB image.\n",
        "      image = self.feature_extractor(image)\n",
        "    \n",
        "    # Flatten the feature map\n",
        "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "    if txt.dtype == tf.string:\n",
        "      # Apply the tokenizer if you get string inputs.\n",
        "      txt = tokenizer(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      txt = dec_layer(inputs=(image, txt))\n",
        "      \n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "UwuI4xAGMFKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_VGG16 = Captioner(tokenizer, feature_extractor=VGG16, output_layer=output_layer,embedding_dim=300,path_to_glove_file=path_to_glove,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ],
      "metadata": {
        "id": "1eljfkBb223T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate captions\n",
        "\n",
        "Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n",
        "\n",
        "Start by downloading a test image:"
      ],
      "metadata": {
        "id": "_UwSi3lHT6T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ],
      "metadata": {
        "id": "JtmkfCMFMHTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To caption an image with this model:\n",
        "\n",
        "- Extract the `img_features`\n",
        "- Initialize the list of output tokens with a `[START]` token.\n",
        "- Pass `img_features` and `tokens` into the model.\n",
        "  - It returns a list of logits.\n",
        "  - Choose the next token based on those logits.  \n",
        "  - Add it to the list of tokens, and continue the loop.\n",
        "  - If it generates an `'[END]'` token, break out of the loop.\n",
        "\n",
        "So add a \"simple\" method to do just that:"
      ],
      "metadata": {
        "id": "IaGjZ0CfUAqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ],
      "metadata": {
        "id": "kY_S4jdvUMEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caption Image using Beam Search of beam size 4"
      ],
      "metadata": {
        "id": "c0V_nqJ4UNOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Τροποποιήσαμε τα ορίσματα της simple_gen ώστε να καλύπτει και το Beam Search."
      ],
      "metadata": {
        "id": "rj-TstdKjuWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, beam_size=4):\n",
        "  initial = self.word_to_index([['[START]']])                        # 'START' token == 3\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])      # extract features from image\n",
        "\n",
        "  tokens_list = [(initial, 0.0)]                  # list of tuples (tokens, propability): stores the top b sentences for each branch (in each iteration)\n",
        "  final_tokens_list = []                          # list of tuples (tokens, propability): final top b selected sentences\n",
        "\n",
        "  remaining_beam_size = beam_size                 # Counter for how many branches in the beam search tree have not terminated yet\n",
        "  for n in range(50):\n",
        "    if remaining_beam_size == 0:\n",
        "      break\n",
        "    new_tokens_list = []                          # top_b tokens for each branch (b*b tokens)\n",
        "    for tokens, prob in tokens_list:              # tokens: [3, 15, 22, 10, ...], prob: float (ex. 0.23)\n",
        "      if tokens[0][-1] == self.word_to_index('[END]'):\n",
        "        remaining_beam_size -= 1\n",
        "        final_tokens_list.append((tokens, prob))\n",
        "        continue\n",
        "      preds = self((img_features, tokens)).numpy()    # generate probabilities for all words given a sentence 'tokens' and an image 'img_features'\n",
        "      preds = preds[:, -1, :]                         # get the probabilities vector only for the last word\n",
        "      top_b_preds = tf.math.top_k(preds, k=remaining_beam_size) # get the b words with the biggest probability\n",
        "      top_b_tokens = top_b_preds.indices              # tokens \n",
        "      top_b_prob = top_b_preds.values                 # probabilities\n",
        "      for i in range(remaining_beam_size):\n",
        "        next_token = [[top_b_tokens[0][i]]]           # next token to be added to the sentence (in one branch)\n",
        "        new_tokens = tf.concat([tokens, next_token], axis=1)\n",
        "        new_log_prob = prob + top_b_prob[0][i]\n",
        "        new_tokens_list.append((new_tokens, new_log_prob))\n",
        "      \n",
        "    tokens_list = sorted(new_tokens_list, key=lambda x: -x[1])[:remaining_beam_size] # Sort the list with b*b sentences and keep the first b\n",
        "\n",
        "  # Return the sentence with the biggest (probability / sentence length) score\n",
        "  tokens, prob = max(final_tokens_list, key=lambda x: x[1]/len(x[0][0, 1:-1]))\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  \n",
        "  return result.numpy().decode()"
      ],
      "metadata": {
        "id": "91_OOmcuMKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"
      ],
      "metadata": {
        "id": "1yfZIK96UH9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model_VGG16.simple_gen(image, beam_size=4)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4gswEq8MMKf",
        "outputId": "08eb7d75-8c20-4f46-abb9-9e6b8b769652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a a\n",
            "a a\n",
            "a a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n"
      ],
      "metadata": {
        "id": "w5JShDxNUUbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Losses and metrics"
      ],
      "metadata": {
        "id": "DgiDgplbUZbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ],
      "metadata": {
        "id": "GQYUJm2FUcMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "TmvTizcvMNQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "Aj8xSuOxUn7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GenerateText - Bean Search Gen"
      ],
      "metadata": {
        "id": "MMcZNvoEq12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, beam_size=4)\n",
        "      print(result)\n",
        "    print()"
      ],
      "metadata": {
        "id": "c6ZYSCf6MOLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = GenerateText()\n",
        "g.model = model_VGG16\n",
        "g.on_epoch_end(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p4N2s-SMO45",
        "outputId": "b1f6ffba-5820-4b60-b403-f5ab2aaa4d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "a a\n",
            "a a\n",
            "a a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "pMdWxKBxMQOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "a_Jcz3zpU-Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ],
      "metadata": {
        "id": "-HtifmQOMRFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_VGG16 = model_VGG16.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C7Ze6x7MR0T",
        "outputId": "6d281eb0-f00d-4554-91a5-2db176c8f849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 5.2482 - masked_acc: 0.1914\n",
            "\n",
            "a man in a man in a man in the\n",
            "a man in a man in a man in the\n",
            "a man in a man in a man in the\n",
            "\n",
            "100/100 [==============================] - 80s 607ms/step - loss: 5.2482 - masked_acc: 0.1914 - val_loss: 5.0346 - val_masked_acc: 0.2217\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.8840 - masked_acc: 0.2420\n",
            "\n",
            "a man in the water\n",
            "a man in the water\n",
            "a man in the water\n",
            "\n",
            "100/100 [==============================] - 36s 361ms/step - loss: 4.8840 - masked_acc: 0.2420 - val_loss: 4.7591 - val_masked_acc: 0.2535\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.7515 - masked_acc: 0.2515\n",
            "\n",
            "a man in the water\n",
            "a man in the water\n",
            "a man in the water\n",
            "\n",
            "100/100 [==============================] - 31s 316ms/step - loss: 4.7515 - masked_acc: 0.2515 - val_loss: 4.6103 - val_masked_acc: 0.2574\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6260 - masked_acc: 0.2593\n",
            "\n",
            "a man in the water\n",
            "a man in the water\n",
            "a man in the water\n",
            "\n",
            "100/100 [==============================] - 32s 318ms/step - loss: 4.6260 - masked_acc: 0.2593 - val_loss: 4.5699 - val_masked_acc: 0.2647\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.5075 - masked_acc: 0.2731\n",
            "\n",
            "a man in a black shirt in the water\n",
            "a man in a black shirt in the water\n",
            "a man in a black shirt in the water\n",
            "\n",
            "100/100 [==============================] - 34s 338ms/step - loss: 4.5075 - masked_acc: 0.2731 - val_loss: 4.4222 - val_masked_acc: 0.2797\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.4453 - masked_acc: 0.2772\n",
            "\n",
            "a man in a blue shirt is playing in the water\n",
            "a man in a blue shirt is playing in the water\n",
            "a man in a blue shirt is playing in the water\n",
            "\n",
            "100/100 [==============================] - 38s 377ms/step - loss: 4.4453 - masked_acc: 0.2772 - val_loss: 4.2719 - val_masked_acc: 0.2959\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3968 - masked_acc: 0.2824\n",
            "\n",
            "a man in a blue shirt is running in the water\n",
            "a man in a blue shirt is running in the water\n",
            "a man in a blue shirt is running in the water\n",
            "\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 4.3968 - masked_acc: 0.2824 - val_loss: 4.2634 - val_masked_acc: 0.2829\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.3148 - masked_acc: 0.2871\n",
            "\n",
            "a man in a blue shirt is running through the water\n",
            "a man in a blue shirt is running through the water\n",
            "a man in a blue shirt is running through the water\n",
            "\n",
            "100/100 [==============================] - 36s 360ms/step - loss: 4.3148 - masked_acc: 0.2871 - val_loss: 4.2289 - val_masked_acc: 0.2984\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2532 - masked_acc: 0.2910\n",
            "\n",
            "a man in a white shirt is riding the water in the water\n",
            "a man in a white shirt is riding the water in the water\n",
            "a man in a white shirt is riding the water in the water\n",
            "\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 4.2532 - masked_acc: 0.2910 - val_loss: 4.1238 - val_masked_acc: 0.2971\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.2251 - masked_acc: 0.2940\n",
            "\n",
            "a man in a blue shirt is standing in the water\n",
            "a man in a blue shirt is standing in the water\n",
            "a man in a blue shirt is standing in the water\n",
            "\n",
            "100/100 [==============================] - 34s 344ms/step - loss: 4.2251 - masked_acc: 0.2940 - val_loss: 4.1860 - val_masked_acc: 0.2934\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1700 - masked_acc: 0.3014\n",
            "\n",
            "a man in a red shirt is running in the water\n",
            "a man in a red shirt is running in the water\n",
            "a man in a red shirt is running in the water\n",
            "\n",
            "100/100 [==============================] - 36s 364ms/step - loss: 4.1700 - masked_acc: 0.3014 - val_loss: 4.1016 - val_masked_acc: 0.2937\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1546 - masked_acc: 0.3002\n",
            "\n",
            "a young boy in a blue shirt is playing in the water\n",
            "a young boy in a blue shirt is playing in the water\n",
            "a young boy in a blue shirt is playing in the water\n",
            "\n",
            "100/100 [==============================] - 36s 364ms/step - loss: 4.1546 - masked_acc: 0.3002 - val_loss: 4.1080 - val_masked_acc: 0.2995\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.1255 - masked_acc: 0.3005\n",
            "\n",
            "a man in a blue shirt is riding on the water\n",
            "a man in a blue shirt is riding on the water\n",
            "a man in a blue shirt is riding on the water\n",
            "\n",
            "100/100 [==============================] - 36s 365ms/step - loss: 4.1255 - masked_acc: 0.3005 - val_loss: 4.0226 - val_masked_acc: 0.3009\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0988 - masked_acc: 0.2988\n",
            "\n",
            "a man in a blue shirt is riding in the water\n",
            "a man in a blue shirt is riding in the water\n",
            "a man in a blue shirt is riding in the water\n",
            "\n",
            "100/100 [==============================] - 35s 349ms/step - loss: 4.0988 - masked_acc: 0.2988 - val_loss: 3.9453 - val_masked_acc: 0.3159\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0727 - masked_acc: 0.2997\n",
            "\n",
            "a man in a blue shirt is riding a water in the water\n",
            "a man in a blue shirt is riding a water in the water\n",
            "a man in a blue shirt is riding a water in the water\n",
            "\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 4.0727 - masked_acc: 0.2997 - val_loss: 3.8968 - val_masked_acc: 0.3173\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.0473 - masked_acc: 0.3050\n",
            "\n",
            "a man in a blue shirt is in the water in the water\n",
            "a man in a blue shirt is in the water in the water\n",
            "a man in a blue shirt is in the water in the water\n",
            "\n",
            "100/100 [==============================] - 36s 362ms/step - loss: 4.0473 - masked_acc: 0.3050 - val_loss: 3.8981 - val_masked_acc: 0.3199\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9756 - masked_acc: 0.3111\n",
            "\n",
            "a man in a blue shirt is in the water\n",
            "a man in a blue shirt is in the water\n",
            "a man in a blue shirt is in the water\n",
            "\n",
            "100/100 [==============================] - 34s 342ms/step - loss: 3.9756 - masked_acc: 0.3111 - val_loss: 3.8780 - val_masked_acc: 0.3141\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9878 - masked_acc: 0.3053\n",
            "\n",
            "a man in the water in the water\n",
            "a man in the water in the water\n",
            "a man in the water in the water\n",
            "\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 3.9878 - masked_acc: 0.3053 - val_loss: 3.9297 - val_masked_acc: 0.3102\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9307 - masked_acc: 0.3102\n",
            "\n",
            "a man in a blue shirt and a red shirt and a red shirt is riding the water in the water\n",
            "a man in a blue shirt and a red shirt and a red shirt is riding the water in the water\n",
            "a man in a blue shirt and a red shirt and a red shirt is riding the water in the water\n",
            "\n",
            "100/100 [==============================] - 43s 437ms/step - loss: 3.9307 - masked_acc: 0.3102 - val_loss: 3.8231 - val_masked_acc: 0.3158\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9514 - masked_acc: 0.3100\n",
            "\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "\n",
            "100/100 [==============================] - 31s 306ms/step - loss: 3.9514 - masked_acc: 0.3100 - val_loss: 3.8330 - val_masked_acc: 0.3185\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.9091 - masked_acc: 0.3124\n",
            "\n",
            "a man in a red shirt and white shirt is riding on the water\n",
            "a man in a red shirt and white shirt is riding on the water\n",
            "a man in a red shirt and white shirt is riding on the water\n",
            "\n",
            "100/100 [==============================] - 39s 393ms/step - loss: 3.9091 - masked_acc: 0.3124 - val_loss: 3.8063 - val_masked_acc: 0.3255\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8880 - masked_acc: 0.3169\n",
            "\n",
            "a young girl in a red shirt is jumping in the water\n",
            "a young girl in a red shirt is jumping in the water\n",
            "a young girl in a red shirt is jumping in the water\n",
            "\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 3.8880 - masked_acc: 0.3169 - val_loss: 3.8178 - val_masked_acc: 0.3085\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8719 - masked_acc: 0.3151\n",
            "\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "\n",
            "100/100 [==============================] - 37s 372ms/step - loss: 3.8719 - masked_acc: 0.3151 - val_loss: 3.7055 - val_masked_acc: 0.3335\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8823 - masked_acc: 0.3143\n",
            "\n",
            "a man in a blue shirt is riding a bike in front of water in the water\n",
            "a man in a blue shirt is riding a bike in front of water in the water\n",
            "a man in a blue shirt is riding a bike in front of water in the water\n",
            "\n",
            "100/100 [==============================] - 38s 382ms/step - loss: 3.8823 - masked_acc: 0.3143 - val_loss: 3.7085 - val_masked_acc: 0.3190\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8141 - masked_acc: 0.3187\n",
            "\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "a young boy in a blue shirt is jumping in the water\n",
            "\n",
            "100/100 [==============================] - 42s 421ms/step - loss: 3.8141 - masked_acc: 0.3187 - val_loss: 3.7087 - val_masked_acc: 0.3247\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8147 - masked_acc: 0.3224\n",
            "\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "\n",
            "100/100 [==============================] - 31s 311ms/step - loss: 3.8147 - masked_acc: 0.3224 - val_loss: 3.7793 - val_masked_acc: 0.3167\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8057 - masked_acc: 0.3231\n",
            "\n",
            "a man in a blue shirt is jumping into the water in the water\n",
            "a man in a blue shirt is jumping into the water in the water\n",
            "a man in a blue shirt is jumping into the water in the water\n",
            "\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 3.8057 - masked_acc: 0.3231 - val_loss: 3.7292 - val_masked_acc: 0.3257\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8143 - masked_acc: 0.3205\n",
            "\n",
            "a man in a red shirt is riding on the water\n",
            "a man in a red shirt is riding on the water\n",
            "a man in a red shirt is riding on the water\n",
            "\n",
            "100/100 [==============================] - 35s 350ms/step - loss: 3.8143 - masked_acc: 0.3205 - val_loss: 3.6262 - val_masked_acc: 0.3323\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.8112 - masked_acc: 0.3223\n",
            "\n",
            "a man in a blue shirt is running in the water\n",
            "a man in a blue shirt is running in the water\n",
            "a man in a blue shirt is running in the water\n",
            "\n",
            "100/100 [==============================] - 35s 351ms/step - loss: 3.8112 - masked_acc: 0.3223 - val_loss: 3.6569 - val_masked_acc: 0.3350\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7124 - masked_acc: 0.3280\n",
            "\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "a young boy in the water in the water\n",
            "\n",
            "100/100 [==============================] - 31s 312ms/step - loss: 3.7124 - masked_acc: 0.3280 - val_loss: 3.7116 - val_masked_acc: 0.3217\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7138 - masked_acc: 0.3260\n",
            "\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "\n",
            "100/100 [==============================] - 37s 371ms/step - loss: 3.7138 - masked_acc: 0.3260 - val_loss: 3.5872 - val_masked_acc: 0.3407\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.7234 - masked_acc: 0.3258\n",
            "\n",
            "a young boy in a swimming pool\n",
            "a young boy in a swimming pool\n",
            "a young boy in a swimming pool\n",
            "\n",
            "100/100 [==============================] - 36s 360ms/step - loss: 3.7234 - masked_acc: 0.3258 - val_loss: 3.5977 - val_masked_acc: 0.3288\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6956 - masked_acc: 0.3270\n",
            "\n",
            "a young boy in a blue shirt is jumping into the water\n",
            "a young boy in a blue shirt is jumping into the water\n",
            "a young boy in a blue shirt is jumping into the water\n",
            "\n",
            "100/100 [==============================] - 39s 391ms/step - loss: 3.6956 - masked_acc: 0.3270 - val_loss: 3.6679 - val_masked_acc: 0.3239\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6511 - masked_acc: 0.3323\n",
            "\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "a young boy in a blue shirt is jumping into the water in the water\n",
            "\n",
            "100/100 [==============================] - 38s 378ms/step - loss: 3.6511 - masked_acc: 0.3323 - val_loss: 3.6580 - val_masked_acc: 0.3310\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6433 - masked_acc: 0.3347\n",
            "\n",
            "a young boy in the water\n",
            "a young boy in the water\n",
            "a young boy in the water\n",
            "\n",
            "100/100 [==============================] - 35s 354ms/step - loss: 3.6433 - masked_acc: 0.3347 - val_loss: 3.6270 - val_masked_acc: 0.3318\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6244 - masked_acc: 0.3259\n",
            "\n",
            "a man in a blue shirt is riding a wave in the water\n",
            "a man in a blue shirt is riding a wave in the water\n",
            "a man in a blue shirt is riding a wave in the water\n",
            "\n",
            "100/100 [==============================] - 35s 355ms/step - loss: 3.6244 - masked_acc: 0.3259 - val_loss: 3.5749 - val_masked_acc: 0.3311\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6240 - masked_acc: 0.3354\n",
            "\n",
            "a young boy in a red shirt is riding a wave in the water\n",
            "a young boy in a red shirt is riding a wave in the water\n",
            "a young boy in a red shirt is riding a wave in the water\n",
            "\n",
            "100/100 [==============================] - 36s 362ms/step - loss: 3.6240 - masked_acc: 0.3354 - val_loss: 3.5503 - val_masked_acc: 0.3368\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.6010 - masked_acc: 0.3327\n",
            "\n",
            "a young boy riding a wave in the water\n",
            "a young boy riding a wave in the water\n",
            "a young boy riding a wave in the water\n",
            "\n",
            "100/100 [==============================] - 31s 315ms/step - loss: 3.6010 - masked_acc: 0.3327 - val_loss: 3.4998 - val_masked_acc: 0.3467\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5762 - masked_acc: 0.3374\n",
            "\n",
            "a young boy in a blue shirt is riding a wave in the water\n",
            "a young boy in a blue shirt is riding a wave in the water\n",
            "a young boy in a blue shirt is riding a wave in the water\n",
            "\n",
            "100/100 [==============================] - 36s 361ms/step - loss: 3.5762 - masked_acc: 0.3374 - val_loss: 3.5575 - val_masked_acc: 0.3406\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5555 - masked_acc: 0.3380\n",
            "\n",
            "a young boy riding a wave in the water\n",
            "a young boy riding a wave in the water\n",
            "a young boy riding a wave in the water\n",
            "\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 3.5555 - masked_acc: 0.3380 - val_loss: 3.5339 - val_masked_acc: 0.3374\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5783 - masked_acc: 0.3354\n",
            "\n",
            "a young man in a black jacket is riding a wave\n",
            "a young man in a black jacket is riding a wave\n",
            "a young man in a black jacket is riding a wave\n",
            "\n",
            "100/100 [==============================] - 36s 356ms/step - loss: 3.5783 - masked_acc: 0.3354 - val_loss: 3.4948 - val_masked_acc: 0.3472\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5989 - masked_acc: 0.3376\n",
            "\n",
            "a young boy in a wetsuit is swimming in the water\n",
            "a young boy in a wetsuit is swimming in the water\n",
            "a young boy in a wetsuit is swimming in the water\n",
            "\n",
            "100/100 [==============================] - 33s 331ms/step - loss: 3.5989 - masked_acc: 0.3376 - val_loss: 3.5198 - val_masked_acc: 0.3341\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5403 - masked_acc: 0.3416\n",
            "\n",
            "a young boy in a swimming pool\n",
            "a young boy in a swimming pool\n",
            "a young boy in a swimming pool\n",
            "\n",
            "100/100 [==============================] - 32s 325ms/step - loss: 3.5403 - masked_acc: 0.3416 - val_loss: 3.4441 - val_masked_acc: 0.3458\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5418 - masked_acc: 0.3390\n",
            "\n",
            "a surfer riding a wave\n",
            "a surfer riding a wave\n",
            "a surfer riding a wave\n",
            "\n",
            "100/100 [==============================] - 31s 316ms/step - loss: 3.5418 - masked_acc: 0.3390 - val_loss: 3.5590 - val_masked_acc: 0.3384\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5620 - masked_acc: 0.3359\n",
            "\n",
            "a surfer riding a wave in the ocean\n",
            "a surfer riding a wave in the ocean\n",
            "a surfer riding a wave in the ocean\n",
            "\n",
            "100/100 [==============================] - 30s 305ms/step - loss: 3.5620 - masked_acc: 0.3359 - val_loss: 3.5210 - val_masked_acc: 0.3450\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5806 - masked_acc: 0.3337\n",
            "\n",
            "a surfer is riding a wave\n",
            "a surfer is riding a wave\n",
            "a surfer is riding a wave\n",
            "\n",
            "100/100 [==============================] - 35s 347ms/step - loss: 3.5806 - masked_acc: 0.3337 - val_loss: 3.5041 - val_masked_acc: 0.3380\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5552 - masked_acc: 0.3359\n",
            "\n",
            "a man in a wetsuit is surfing in the ocean\n",
            "a man in a wetsuit is surfing in the ocean\n",
            "a man in a wetsuit is surfing in the ocean\n",
            "\n",
            "100/100 [==============================] - 38s 377ms/step - loss: 3.5552 - masked_acc: 0.3359 - val_loss: 3.4862 - val_masked_acc: 0.3488\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 3.5444 - masked_acc: 0.3368\n",
            "\n",
            "a surfer is surfing in the water\n",
            "a surfer is surfing in the water\n",
            "a surfer is surfing in the water\n",
            "\n",
            "100/100 [==============================] - 33s 330ms/step - loss: 3.5444 - masked_acc: 0.3368 - val_loss: 3.4467 - val_masked_acc: 0.3403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bleu Scores"
      ],
      "metadata": {
        "id": "KamIJwsYVDXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = (0.4, 0.3, 0.2, 0.1)\n",
        "smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "def sentence_bleu_calc(references,hyp):\n",
        "  return(nltk.translate.bleu_score.sentence_bleu(\n",
        "                                          references, \n",
        "                                          hyp, \n",
        "                                          weights = weights, \n",
        "                                          smoothing_function=smoothing_function))\n",
        "\n",
        "def corpus_bleu_calc(list_of_references, list_of_hypotheses):\n",
        "  return(nltk.translate.bleu_score.corpus_bleu(list_of_references, \n",
        "                                               list_of_hypotheses, \n",
        "                                               weights = weights, \n",
        "                                               smoothing_function=smoothing_function))"
      ],
      "metadata": {
        "id": "QwpGsKdAO2rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_references = [row[1] for row in test_captions]\n",
        "references = [[s.split()[:-1] for s in sublist] for sublist in image_references]\n",
        "image_paths = [row[0] for row in test_captions]"
      ],
      "metadata": {
        "id": "YIi6jpVSO6Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bleu score for a singular image"
      ],
      "metadata": {
        "id": "zCpRbYfVVMsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myimage = load_image(image_paths[0])\n",
        "myhyp = model_VGG16.simple_gen(myimage).split()\n",
        "print(myhyp)\n",
        "sentence_bleu_calc(references[0],myhyp)"
      ],
      "metadata": {
        "id": "i7MrzuqRPDVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2122e338-6632-42e7-8022-b407988f8dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'running', 'through', 'a', 'field']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2315817779303589"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bleu Score for a larger dataset"
      ],
      "metadata": {
        "id": "PGnN7UTZVPz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VGG16_hypothesis_captions = []\n",
        "\n",
        "for i_path in image_paths[:1000]:\n",
        "  i_image = load_image(i_path)\n",
        "  i_hyp = model_VGG16.simple_gen(i_image,beam_size=4).split()\n",
        "  VGG16_hypothesis_captions.append(i_hyp)"
      ],
      "metadata": {
        "id": "cRSu7-zVPHZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu_calc(references[:1000], VGG16_hypothesis_captions)"
      ],
      "metadata": {
        "id": "xaMNYBuUPKCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb02181-9a5f-4e65-c792-20fffb613461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19638853668023262"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gLjfvXhQkP1X",
        "PnXhaKisv0DU",
        "1ncWplQjwztZ",
        "BEucB8xG6bAn",
        "u15iab5LBwsg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}